{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "ecf5722fdaf1897a315d257d89d94520bfcaa453217d5becf09b39e73618b0de"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-41ea56f79ebc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mmax_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Helsinki-NLP/opus-mt-en-zh\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_max_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAutoModelForSeq2SeqLM\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Helsinki-NLP/opus-mt-en-zh\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    436\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mtokenizer_class_py\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 438\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mtokenizer_class_py\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    439\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m                     raise ValueError(\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1670\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1671\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1672\u001b[1;33m                     resolved_vocab_files[file_id] = cached_path(\n\u001b[0m\u001b[0;32m   1673\u001b[0m                         \u001b[0mfile_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1674\u001b[0m                         \u001b[0mcache_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\transformers\\file_utils.py\u001b[0m in \u001b[0;36mcached_path\u001b[1;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[0;32m   1269\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mis_remote_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl_or_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1270\u001b[0m         \u001b[1;31m# URL, so get it from the cache (downloading if necessary)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1271\u001b[1;33m         output_path = get_from_cache(\n\u001b[0m\u001b[0;32m   1272\u001b[0m             \u001b[0murl_or_filename\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1273\u001b[0m             \u001b[0mcache_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\transformers\\file_utils.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[1;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001b[0m\n\u001b[0;32m   1492\u001b[0m                     )\n\u001b[0;32m   1493\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1494\u001b[1;33m                     raise ValueError(\n\u001b[0m\u001b[0;32m   1495\u001b[0m                         \u001b[1;34m\"Connection error, and we cannot find the requested files in the cached path.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1496\u001b[0m                         \u001b[1;34m\" Please try again or make sure your Internet connection is on.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "smooth = SmoothingFunction()\n",
    "\n",
    "max_length = 128\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-zh\", model_max_length = max_length)\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-zh\")\n",
    "\n",
    "'''\n",
    "class BertTransModel(nn.Module):\n",
    "    def __init__(self):\n",
    "\n",
    "        super(BertTransModel,self).__init__()\n",
    "\n",
    "        self.bert = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-zh\")\n",
    "        \n",
    "    def forward(self, ids, mask, tgt, mask1):\n",
    "\n",
    "        out, _ = self.bert(input_ids = ids, attention_mask = mask, decoder_ids = tgt)\n",
    "\n",
    "        return out\n",
    "'''\n",
    "\n",
    "#mymodel = BertTransModel()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#mymodel.to(device)\n",
    "\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('C:/Users/z1055/Desktop/ClassDocument/NLP/Experiment6/train.csv',sep = ',',names = ['text','label'],skiprows = 1)\n",
    "\n",
    "print(train_df.shape)\n",
    "\n",
    "count = train_df.shape[0]\n",
    "\n",
    "train_df.head()\n",
    "\n",
    "sentences = list(train_df['text'])\n",
    "    \n",
    "targets = list(train_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_layers(model,freeze_list):\n",
    "\n",
    "    for name, child in model.named_children():\n",
    "\n",
    "        if name not in ['model']:\n",
    "\n",
    "            continue\n",
    "\n",
    "        for name1, child1 in child.named_children():\n",
    "\n",
    "            if name1 not in ['decoder']:\n",
    "\n",
    "                continue\n",
    "\n",
    "            for name2, child2 in child1.named_children():\n",
    "\n",
    "                if name2 not in ['layers']:\n",
    "\n",
    "                    continue\n",
    "\n",
    "                for name3, child3 in child2.named_children():\n",
    "\n",
    "                    if(int(name3) not in freeze_list):\n",
    "                        \n",
    "                        continue\n",
    "\n",
    "                    for para in child3.parameters():\n",
    "\n",
    "                        para.requires_grad = False\n",
    "\n",
    "freeze_layers(model,[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3255: FutureWarning: `prepare_seq2seq_batch` is deprecated and will be removed in version 5 of 🤗 Transformers. Use the regular `__call__` method to prepare your inputs and the tokenizer under the `with_target_tokenizer` context manager to prepare your targets. See the documentation of your specific tokenizer for more details\n",
      "  warnings.warn(\n",
      "[Epoch = 0 Iternum = 100 Loss = 1.414109]\n",
      "[Epoch = 0 Iternum = 200 Loss = 1.198318]\n",
      "[Epoch = 0 Iternum = 300 Loss = 1.640920]\n",
      "[Epoch = 0 Iternum = 400 Loss = 1.605938]\n",
      "[Epoch = 0 Iternum = 500 Loss = 2.023006]\n",
      "[Epoch = 0 Iternum = 600 Loss = 1.233098]\n",
      "[Epoch = 0 Iternum = 700 Loss = 1.479905]\n",
      "[Epoch = 0 Iternum = 800 Loss = 1.745520]\n",
      "[Epoch = 0 Iternum = 900 Loss = 1.472933]\n",
      "[Epoch = 0 Iternum = 1000 Loss = 1.366561]\n",
      "[Epoch = 0 Iternum = 1100 Loss = 1.526798]\n",
      "[Epoch = 0 Iternum = 1200 Loss = 1.758339]\n",
      "[Epoch = 0 Iternum = 1300 Loss = 1.574931]\n",
      "[Epoch = 0 Iternum = 1400 Loss = 1.516887]\n",
      "[Epoch = 0 Iternum = 1500 Loss = 1.446774]\n",
      "[Epoch = 0 Iternum = 1600 Loss = 0.560364]\n",
      "[Epoch = 0 Iternum = 1700 Loss = 0.647595]\n",
      "[Epoch = 0 Iternum = 1800 Loss = 0.474386]\n",
      "[Epoch = 0 Iternum = 1900 Loss = 0.544913]\n",
      "[Epoch = 0 Iternum = 2000 Loss = 0.650621]\n",
      "[Epoch = 0 Iternum = 2100 Loss = 0.541600]\n",
      "[Epoch = 1 Iternum = 100 Loss = 1.247902]\n",
      "[Epoch = 1 Iternum = 200 Loss = 1.136950]\n",
      "[Epoch = 1 Iternum = 300 Loss = 1.536870]\n",
      "[Epoch = 1 Iternum = 400 Loss = 1.491487]\n",
      "[Epoch = 1 Iternum = 500 Loss = 1.910765]\n",
      "[Epoch = 1 Iternum = 600 Loss = 1.135698]\n",
      "[Epoch = 1 Iternum = 700 Loss = 1.400180]\n",
      "[Epoch = 1 Iternum = 800 Loss = 1.669582]\n",
      "[Epoch = 1 Iternum = 900 Loss = 1.386277]\n",
      "[Epoch = 1 Iternum = 1000 Loss = 1.318184]\n",
      "[Epoch = 1 Iternum = 1100 Loss = 1.495965]\n",
      "[Epoch = 1 Iternum = 1200 Loss = 1.670906]\n",
      "[Epoch = 1 Iternum = 1300 Loss = 1.543203]\n",
      "[Epoch = 1 Iternum = 1400 Loss = 1.476052]\n",
      "[Epoch = 1 Iternum = 1500 Loss = 1.340038]\n",
      "[Epoch = 1 Iternum = 1600 Loss = 0.478571]\n",
      "[Epoch = 1 Iternum = 1700 Loss = 0.575494]\n",
      "[Epoch = 1 Iternum = 1800 Loss = 0.459506]\n",
      "[Epoch = 1 Iternum = 1900 Loss = 0.492130]\n",
      "[Epoch = 1 Iternum = 2000 Loss = 0.580253]\n",
      "[Epoch = 1 Iternum = 2100 Loss = 0.483902]\n",
      "[Epoch = 2 Iternum = 100 Loss = 1.181676]\n",
      "[Epoch = 2 Iternum = 200 Loss = 1.111024]\n",
      "[Epoch = 2 Iternum = 300 Loss = 1.501039]\n",
      "[Epoch = 2 Iternum = 400 Loss = 1.421273]\n",
      "[Epoch = 2 Iternum = 500 Loss = 1.824160]\n",
      "[Epoch = 2 Iternum = 600 Loss = 1.070454]\n",
      "[Epoch = 2 Iternum = 700 Loss = 1.361219]\n",
      "[Epoch = 2 Iternum = 800 Loss = 1.587106]\n",
      "[Epoch = 2 Iternum = 900 Loss = 1.362475]\n",
      "[Epoch = 2 Iternum = 1000 Loss = 1.264410]\n",
      "[Epoch = 2 Iternum = 1100 Loss = 1.436920]\n",
      "[Epoch = 2 Iternum = 1200 Loss = 1.616191]\n",
      "[Epoch = 2 Iternum = 1300 Loss = 1.502106]\n",
      "[Epoch = 2 Iternum = 1400 Loss = 1.470023]\n",
      "[Epoch = 2 Iternum = 1500 Loss = 1.300476]\n",
      "[Epoch = 2 Iternum = 1600 Loss = 0.427795]\n",
      "[Epoch = 2 Iternum = 1700 Loss = 0.515658]\n",
      "[Epoch = 2 Iternum = 1800 Loss = 0.393217]\n",
      "[Epoch = 2 Iternum = 1900 Loss = 0.440881]\n",
      "[Epoch = 2 Iternum = 2000 Loss = 0.550581]\n",
      "[Epoch = 2 Iternum = 2100 Loss = 0.444931]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from torch import optim\n",
    "\n",
    "from transformers import AdamW\n",
    "#optimizer = optim.Adam(mymodel.parameters(), lr = 0.0001)\n",
    "#optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n",
    "optimizer = AdamW(model.parameters(), lr = 0.00001)\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    train_loss = 0.0\n",
    "\n",
    "    rangeNum = int(count / batch_size + 1)\n",
    "\n",
    "    for val in range(rangeNum):\n",
    "\n",
    "        if val * batch_size >= count:\n",
    "            break\n",
    "\n",
    "        if val == rangeNum - 1:\n",
    "\n",
    "            src_batch = tokenizer.prepare_seq2seq_batch(src_texts = sentences[val * batch_size:],padding=True)\n",
    "            \n",
    "            with tokenizer.as_target_tokenizer():\n",
    "\n",
    "                tgt_batch = tokenizer.prepare_seq2seq_batch(src_texts = targets[val * batch_size:],padding=True)\n",
    "\n",
    "        else:\n",
    "\n",
    "            src_batch = tokenizer.prepare_seq2seq_batch(src_texts = sentences[val * batch_size:batch_size * (val + 1)], padding=True)\n",
    "\n",
    "            with tokenizer.as_target_tokenizer():\n",
    "\n",
    "                tgt_batch = tokenizer.prepare_seq2seq_batch(src_texts = targets[val * batch_size:batch_size * (val + 1)], padding=True)\n",
    "\n",
    "        src_batch[\"input_ids\"] = torch.tensor(src_batch[\"input_ids\"][:][:max_length]).to(device).detach()\n",
    "\n",
    "        tgt_batch[\"input_ids\"] = torch.tensor(tgt_batch[\"input_ids\"][:][:max_length]).to(device).detach()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = model(input_ids = src_batch[\"input_ids\"],  labels = tgt_batch[\"input_ids\"], ).loss\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if (val + 1) % 100 == 0:\n",
    "\n",
    "            print('[Epoch = %d Iternum = %d Loss = %f]'%(epoch,val + 1, loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bleu(pred_trgs, trgs):\n",
    "\n",
    "    res = 0\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for candidate,reference in zip(pred_trgs,trgs):\n",
    "\n",
    "        count = count + 1\n",
    "\n",
    "        #score = sentence_bleu([reference], candidate,smoothing_function = smooth.method1,weights=(0.25, 0.25, 0.25, 0.25))\n",
    "\n",
    "        score = sentence_bleu([reference], candidate, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "\n",
    "\n",
    "        res = res + score\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(file_dir,batch_size,tokenizer,model,num_beams):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    pred = ''\n",
    "\n",
    "    src_list = []\n",
    "\n",
    "    tgt_list = []\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    with open(file_dir,'r',encoding = 'utf-8') as sentence_file:\n",
    "\n",
    "        for sentence in sentence_file.read().split('\\n'):\n",
    "\n",
    "            count = count + 1\n",
    "\n",
    "            if(count % 2 == 0):\n",
    "\n",
    "                tgt_list.append(sentence)\n",
    "\n",
    "            else:\n",
    "\n",
    "                src_list.append(sentence)\n",
    "\n",
    "    print('Processing Complete')\n",
    "\n",
    "    rangeNum = int(count / batch_size / 2 + 1)\n",
    "\n",
    "    score = 0\n",
    "\n",
    "    for val in range(rangeNum):\n",
    "\n",
    "        if val * batch_size >= count / 2:\n",
    "\n",
    "            break\n",
    "\n",
    "        if val == rangeNum - 1:\n",
    "\n",
    "            batch = tokenizer.prepare_seq2seq_batch(src_texts=src_list[val * batch_size:])\n",
    "\n",
    "        else:\n",
    "\n",
    "            batch = tokenizer.prepare_seq2seq_batch(src_texts=src_list[val * batch_size:batch_size * (val + 1)])\n",
    "\n",
    "        batch[\"input_ids\"] = torch.tensor(batch[\"input_ids\"][:][:max_length]).to(device).detach()\n",
    "\n",
    "        batch[\"attention_mask\"] = torch.tensor(batch[\"attention_mask\"][:][:max_length]).to(device).detach()\n",
    "\n",
    "        translation = model.generate(**batch, num_beams = num_beams)\n",
    "\n",
    "        result = tokenizer.batch_decode(translation, skip_special_tokens=True, num_beams = num_beams)\n",
    "\n",
    "        if val == rangeNum - 1:\n",
    "\n",
    "            score = score + calculate_bleu(result,tgt_list[val * batch_size:])\n",
    "\n",
    "        else:\n",
    "\n",
    "            score = score + calculate_bleu(result,tgt_list[val * batch_size:batch_size * (val + 1)])\n",
    "\n",
    "        if val % 5 == 0:\n",
    "\n",
    "            print('[Iternum = %d BLEU SCORE = %f]'%(val,score * 100.0 / ((val + 1) * batch_size)))\n",
    "\n",
    "    return score * 100.0 / count * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Processing Complete\n",
      "[Iternum = 0 BLEU SCORE = 16.173985]\n",
      "[Iternum = 5 BLEU SCORE = 16.348367]\n",
      "[Iternum = 10 BLEU SCORE = 16.271617]\n",
      "[Iternum = 15 BLEU SCORE = 17.510386]\n",
      "[Iternum = 20 BLEU SCORE = 19.046815]\n",
      "[Iternum = 25 BLEU SCORE = 19.529813]\n",
      "[Iternum = 30 BLEU SCORE = 19.122892]\n",
      "[Iternum = 35 BLEU SCORE = 18.741071]\n",
      "[Iternum = 40 BLEU SCORE = 19.237890]\n",
      "[Iternum = 45 BLEU SCORE = 19.680164]\n",
      "[Iternum = 50 BLEU SCORE = 19.947722]\n",
      "[Iternum = 55 BLEU SCORE = 19.630131]\n",
      "[Iternum = 60 BLEU SCORE = 19.514640]\n",
      "[Iternum = 65 BLEU SCORE = 19.130647]\n",
      "[Iternum = 70 BLEU SCORE = 19.307333]\n",
      "[Iternum = 75 BLEU SCORE = 19.219137]\n",
      "[Iternum = 80 BLEU SCORE = 18.828400]\n",
      "[Iternum = 85 BLEU SCORE = 18.697297]\n",
      "[Iternum = 90 BLEU SCORE = 18.699316]\n",
      "[Iternum = 95 BLEU SCORE = 18.926841]\n",
      "[Iternum = 100 BLEU SCORE = 18.840455]\n",
      "[Iternum = 105 BLEU SCORE = 18.897204]\n",
      "[Iternum = 110 BLEU SCORE = 18.928305]\n",
      "[Iternum = 115 BLEU SCORE = 19.013273]\n",
      "[Iternum = 120 BLEU SCORE = 18.997963]\n",
      "[Iternum = 125 BLEU SCORE = 18.878625]\n",
      "[Iternum = 130 BLEU SCORE = 18.842097]\n",
      "[Iternum = 135 BLEU SCORE = 18.818136]\n",
      "[Iternum = 140 BLEU SCORE = 18.516676]\n",
      "[Iternum = 145 BLEU SCORE = 18.399979]\n",
      "[Iternum = 150 BLEU SCORE = 18.176051]\n",
      "[Iternum = 155 BLEU SCORE = 18.213617]\n",
      "[Iternum = 160 BLEU SCORE = 18.203516]\n",
      "[Iternum = 165 BLEU SCORE = 18.106449]\n",
      "[Iternum = 170 BLEU SCORE = 18.011190]\n",
      "[Iternum = 175 BLEU SCORE = 18.003610]\n",
      "[Iternum = 180 BLEU SCORE = 18.084297]\n",
      "[Iternum = 185 BLEU SCORE = 17.843631]\n",
      "[Iternum = 190 BLEU SCORE = 17.673962]\n",
      "[Iternum = 195 BLEU SCORE = 17.548598]\n",
      "[Iternum = 200 BLEU SCORE = 17.589569]\n",
      "[Iternum = 205 BLEU SCORE = 17.588904]\n",
      "[Iternum = 210 BLEU SCORE = 17.378422]\n",
      "[Iternum = 215 BLEU SCORE = 17.372596]\n",
      "[Iternum = 220 BLEU SCORE = 17.481210]\n",
      "[Iternum = 225 BLEU SCORE = 17.501226]\n",
      "[Iternum = 230 BLEU SCORE = 17.498925]\n",
      "[Iternum = 235 BLEU SCORE = 17.457013]\n",
      "[Iternum = 240 BLEU SCORE = 17.456753]\n",
      "[Iternum = 245 BLEU SCORE = 17.476293]\n",
      "[Iternum = 250 BLEU SCORE = 17.504772]\n",
      "[Iternum = 255 BLEU SCORE = 17.429783]\n",
      "[Iternum = 260 BLEU SCORE = 17.492747]\n",
      "[Iternum = 265 BLEU SCORE = 17.486779]\n",
      "[Iternum = 270 BLEU SCORE = 17.396336]\n",
      "[Iternum = 275 BLEU SCORE = 17.513722]\n",
      "[Iternum = 280 BLEU SCORE = 17.442368]\n",
      "[Iternum = 285 BLEU SCORE = 17.509113]\n",
      "[Iternum = 290 BLEU SCORE = 17.564277]\n",
      "[Iternum = 295 BLEU SCORE = 17.614671]\n",
      "[Iternum = 300 BLEU SCORE = 17.596985]\n",
      "[Iternum = 305 BLEU SCORE = 17.602866]\n",
      "[Iternum = 310 BLEU SCORE = 17.718890]\n",
      "[Iternum = 315 BLEU SCORE = 17.969364]\n",
      "[Iternum = 320 BLEU SCORE = 18.532636]\n",
      "[Iternum = 325 BLEU SCORE = 18.978233]\n",
      "[Iternum = 330 BLEU SCORE = 19.601975]\n",
      "[Iternum = 335 BLEU SCORE = 19.946859]\n",
      "[Iternum = 340 BLEU SCORE = 20.215214]\n",
      "[Iternum = 345 BLEU SCORE = 20.469887]\n",
      "[Iternum = 350 BLEU SCORE = 20.849919]\n",
      "[Iternum = 355 BLEU SCORE = 21.087802]\n",
      "[Iternum = 360 BLEU SCORE = 21.515245]\n",
      "[Iternum = 365 BLEU SCORE = 21.922887]\n",
      "[Iternum = 370 BLEU SCORE = 22.234214]\n",
      "[Iternum = 375 BLEU SCORE = 22.423624]\n",
      "[Iternum = 380 BLEU SCORE = 22.811378]\n",
      "[Iternum = 385 BLEU SCORE = 23.027534]\n",
      "Res =  23.03221775442412\n"
     ]
    }
   ],
   "source": [
    "save_directory = \"C:/Users/z1055/Desktop/ClassDocument/NLP/Experiment6/models/best_model\"\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
    "\n",
    "#model = AutoModelForSeq2SeqLM.from_pretrained(save_directory).to(device)\n",
    "\n",
    "res = evaluate(file_dir = 'C:/Users/z1055/Desktop/ClassDocument/NLP/Experiment6/valid.txt',batch_size = 8,tokenizer = tokenizer,model = model,num_beams = 10)\n",
    "\n",
    "print('Res = ',res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_directory = \"C:/Users/z1055/Desktop/ClassDocument/NLP/Experiment6/models/best_model\"\n",
    "\n",
    "#tokenizer.save_pretrained(save_directory)\n",
    "\n",
    "#model.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred(file_dir,pred_file_dir,batch_size,tokenizer,model,num_beams):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    pred = ''\n",
    "\n",
    "    src_list = []\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    with open(file_dir,'r',encoding = 'utf-8') as sentence_file:\n",
    "\n",
    "        for sentence in sentence_file.read().split('\\n'):\n",
    "\n",
    "                src_list.append(sentence)\n",
    "\n",
    "                count = count + 1\n",
    "\n",
    "    print('Processing Complete')\n",
    "\n",
    "    rangeNum = int(count / batch_size + 1)\n",
    "\n",
    "    score = 0\n",
    "\n",
    "    with open(pred_file_dir,'w') as sentence_file:\n",
    "\n",
    "        for val in range(rangeNum):\n",
    "\n",
    "            if val * batch_size >= count:\n",
    "\n",
    "                break\n",
    "\n",
    "            if val == rangeNum - 1:\n",
    "\n",
    "                batch = tokenizer.prepare_seq2seq_batch(src_texts=src_list[val * batch_size:])\n",
    "\n",
    "            else:\n",
    "\n",
    "                batch = tokenizer.prepare_seq2seq_batch(src_texts=src_list[val * batch_size:batch_size * (val + 1)])\n",
    "\n",
    "            batch[\"input_ids\"] = torch.tensor(batch[\"input_ids\"][:][:max_length]).to(device).detach()\n",
    "\n",
    "            batch[\"attention_mask\"] = torch.tensor(batch[\"attention_mask\"][:][:max_length]).to(device).detach()\n",
    "\n",
    "            translation = model.generate(**batch, num_beams = num_beams)\n",
    "\n",
    "            result = tokenizer.batch_decode(translation, skip_special_tokens=True, num_beams = num_beams)\n",
    "\n",
    "            for sentence_result in result:\n",
    "\n",
    "                sentence_file.write(sentence_result + '\\n')\n",
    "\n",
    "            print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Processing Complete\n",
      "['我们喜欢踢足球。', '我们尽了一切努力去帮助他。', '嘿,你的狗咬了我。', '汤姆现在在一家银行工作。', '我注意到她戴着一顶新帽子。', '你用什么打开的?', '我们的棒球队很强壮。', '你想知道为什么我对汤姆说谎吗?']\n",
      "['别听她的。', '他娶了一个空姐。', '什么是爱?', '汤姆这次做得比上次好。', '恭喜!', '还剩多少时间?', '我们两个是学生。', '如果你住在大旅馆,你可以使用他们的游泳池。']\n",
      "['你能忍受他的行为举止吗?', '这个城市的人口大约是10万。']\n"
     ]
    }
   ],
   "source": [
    "save_directory = \"C:/Users/z1055/Desktop/ClassDocument/NLP/Experiment6/models/best_model\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(save_directory).to(device)\n",
    "\n",
    "test_file_dir = 'C:/Users/z1055/Desktop/ClassDocument/NLP/Experiment6/eval/test1.txt'\n",
    "\n",
    "pred_file_dir = 'C:/Users/z1055/Desktop/ClassDocument/NLP/Experiment6/eval/pred1.txt'\n",
    "\n",
    "get_pred(file_dir = test_file_dir, pred_file_dir = pred_file_dir, batch_size = 8, tokenizer = tokenizer, model = model, num_beams = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}